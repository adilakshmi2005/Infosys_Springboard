# -*- coding: utf-8 -*-
"""milestone1_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qezDmKvKuubzWOv6YleTJeB8m9QIbRYX
"""

# 1) Install dependencies (run once)
!pip install -q sentence-transformers transformers umap-learn scikit-learn matplotlib pandas seaborn asttokens

# 2) Imports
import os, io, tokenize, ast
from pathlib import Path
import pandas as pd

# 3) Create snippets folder and write example snippets (replace/extend these with your own later)
os.makedirs('snippets', exist_ok=True)

examples = {
"snippet01_add.py": """def add(a, b):\n    return a + b\n""",
"snippet02_counter_class.py": """class Counter:\n    def __init__(self):\n        self.c = 0\n    def inc(self):\n        self.c += 1\n    def value(self):\n        return self.c\n""",
"snippet03_factorial_recursive.py": """def factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n-1)\n""",
"snippet04_file_io.py": """import json\n\ndef save_json(path, data):\n    with open(path,'w') as f:\n        json.dump(data, f)\n""",
"snippet05_list_comp.py": """def squares(n):\n    return [i*i for i in range(n)]\n""",
"snippet06_decorator.py": """def log(fn):\n    def wrapper(*a, **k):\n        print('calling', fn.__name__)\n        return fn(*a, **k)\n    return wrapper\n\n@log\ndef greet(name):\n    return f'Hello {name}'\n""",
"snippet07_async_gen.py": """import asyncio\n\nasync def fetch(x):\n    await asyncio.sleep(0.01)\n    return x*2\n""",
"snippet08_nested_functions.py": """def outer(x):\n    def inner(y):\n        return x + y\n    return inner\n""",
"snippet09_comprehension_map_filter.py": """def process(nums):\n    return list(map(lambda x: x*2, filter(lambda x: x%2==0, nums)))\n""",
"snippet10_try_except.py": """def safe_div(a, b):\n    try:\n        return a / b\n    except ZeroDivisionError:\n        return None\n"""
}

for fname, code in examples.items():
    with open(os.path.join('snippets', fname), 'w') as f:
        f.write(code)

# 4) Load and preview snippets
snippet_files = sorted(Path('snippets').glob('*.py'))
snippets = []
for p in snippet_files:
    text = p.read_text()
    first_line = text.splitlines()[0] if text.splitlines() else ""
    snippets.append({'filename': p.name, 'lines': len(text.splitlines()), 'first_line': first_line})

df = pd.DataFrame(snippets)
print("Snippets found:", len(df))
display(df)

import ast

def parse_code_features(code):
    tree = ast.parse(code)
    features = {
        "n_functions": 0,
        "n_classes": 0,
        "imports": [],
        "function_names": [],
        "class_names": [],
        "has_recursion": False,
        "has_list_comp": False,
        "lines": len(code.splitlines())
    }

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            features["n_functions"] += 1
            features["function_names"].append(node.name)

            # Detect recursion (function calls itself)
            for child in ast.walk(node):
                if isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                    if child.func.id == node.name:
                        features["has_recursion"] = True

        elif isinstance(node, ast.ClassDef):
            features["n_classes"] += 1
            features["class_names"].append(node.name)

        elif isinstance(node, ast.Import):
            for n in node.names:
                features["imports"].append(n.name)

        elif isinstance(node, ast.ImportFrom):
            base = (node.module or "")
            for n in node.names:
                features["imports"].append(base + "." + n.name if base else n.name)

        elif isinstance(node, ast.ListComp):
            features["has_list_comp"] = True

    return features


# --- Parse all snippets and display summary ---
results = []
for s in snippets:
    code = Path('snippets') / s['filename']
    s['features'] = parse_code_features(code.read_text())
    row = {"file": s['filename'], **s['features']}
    results.append(row)

df_features = pd.DataFrame(results)
display(df_features)

from transformers import AutoTokenizer

# ---------- A) Lexical Tokenization (same as before) ----------
def python_lex_tokens(code):
    tokens = []
    try:
        g = tokenize.generate_tokens(io.StringIO(code).readline)
        for toknum, tokval, start, end, line in g:
            tokens.append((tokenize.tok_name[toknum], tokval))
    except Exception as e:
        tokens = []
    return tokens

# Reload all snippets
snippet_files = sorted(Path('snippets').glob('*.py'))
snippets = [{"filename": p.name, "code": p.read_text()} for p in snippet_files]

print("Example lexical tokens from first snippet:\n")
print(python_lex_tokens(snippets[0]["code"])[:15])

# ---------- âœ… FIXED MODEL NAMES ----------
model_names = {
    "MiniLM": "sentence-transformers/all-MiniLM-L6-v2",
    "DistilRoBERTa": "sentence-transformers/stsb-distilroberta-base-v2",
    "MPNet": "sentence-transformers/all-mpnet-base-v2"
}

# ---------- Load Tokenizers ----------
model_tokenizers = {}
for k, model_name in model_names.items():
    print(f"Loading tokenizer for {k} ({model_name})...")
    model_tokenizers[k] = AutoTokenizer.from_pretrained(model_name)

# ---------- Token Count Comparison ----------
rows = []
for s in snippets:
    code = s["code"]
    lex_tokens = python_lex_tokens(code)
    row = {"file": s["filename"], "lex_tokens": len(lex_tokens)}
    for k, tok in model_tokenizers.items():
        tokens = tok.tokenize(code)
        row[f"{k}_tokens"] = len(tokens)
    rows.append(row)

df_tokens = pd.DataFrame(rows)
display(df_tokens)

from sentence_transformers import SentenceTransformer
import numpy as np
import os

# Folder to store embeddings
os.makedirs('artifacts', exist_ok=True)

# ---------- A) Load pretrained models ----------
model_names = {
    "MiniLM": "all-MiniLM-L6-v2",
    "DistilRoBERTa": "stsb-distilroberta-base-v2",
    "MPNet": "all-mpnet-base-v2"
}

models = {}
for k, model_id in model_names.items():
    print(f"Loading {k} model...")
    models[k] = SentenceTransformer(model_id)

# ---------- B) Encode all snippets ----------
codes = [s['code'] for s in snippets]
embeddings = {}

for k, model in models.items():
    print(f"\nEncoding with {k} ...")
    emb = model.encode(codes, convert_to_numpy=True, show_progress_bar=True)
    embeddings[k] = emb
    print(f"{k} embeddings shape:", emb.shape)

# ---------- C) Save embeddings for reproducibility ----------
for k, emb in embeddings.items():
    np.save(f"artifacts/embeddings_{k}.npy", emb)
    print(f"Saved: artifacts/embeddings_{k}.npy")

from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# ---------- A) Compute cosine similarity matrices ----------
def similarity_matrix(emb):
    return cosine_similarity(emb)

sim_mats = {k: similarity_matrix(v) for k, v in embeddings.items()}

# ---------- B) Visualize a sample matrix ----------
plt.figure(figsize=(6, 5))
plt.imshow(sim_mats['MiniLM'], cmap='viridis', interpolation='nearest')
plt.title('MiniLM - Cosine Similarity Matrix')
plt.colorbar(label='Cosine Similarity')
plt.xlabel('Snippet Index')
plt.ylabel('Snippet Index')
plt.show()

# ---------- C) Compare models using Spearman correlation ----------
pairs = []
names = list(sim_mats.keys())

for i in range(len(names)):
    for j in range(i + 1, len(names)):
        a = sim_mats[names[i]]
        b = sim_mats[names[j]]
        # take only upper-triangle values (to avoid duplicates)
        iu = np.triu_indices_from(a, k=1)
        ra, rb = a[iu], b[iu]
        corr, _ = spearmanr(ra, rb)
        pairs.append({
            "Model A": names[i],
            "Model B": names[j],
            "Spearman Correlation": round(corr, 4)
        })

df_corr = pd.DataFrame(pairs)
display(df_corr)

import umap
import matplotlib.pyplot as plt
import numpy as np

# ---------- A) Define 2D projection function ----------
def plot_2d_embeddings(emb, title, labels=None):
    reducer = umap.UMAP(random_state=42, n_neighbors=5, min_dist=0.3)
    emb_2d = reducer.fit_transform(emb)

    plt.figure(figsize=(7, 5))
    scatter = plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c='skyblue', s=70, edgecolors='black')

    for i, txt in enumerate([s['filename'] for s in snippets]):
        plt.annotate(txt.replace(".py",""), (emb_2d[i, 0], emb_2d[i, 1]), fontsize=8)

    plt.title(title)
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.show()


# ---------- B) Plot embeddings for each model ----------
for k, emb in embeddings.items():
    print(f"ðŸ“Š Visualizing {k} embeddings...")
    plot_2d_embeddings(emb, f"{k} Embeddings (UMAP Projection)")

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
import pandas as pd

# ---------- A) Choose number of clusters ----------
# You can choose based on intuition; here we use 4 for diversity
num_clusters = 4

cluster_labels = {}
for name, emb in embeddings.items():
    km = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
    cluster_labels[name] = km.fit_predict(emb)
    print(f"{name}: cluster labels -> {cluster_labels[name]}")

# ---------- B) Compare models with Adjusted Rand Index ----------
names = list(cluster_labels.keys())
ari_results = []

for i in range(len(names)):
    for j in range(i + 1, len(names)):
        ari = adjusted_rand_score(cluster_labels[names[i]], cluster_labels[names[j]])
        ari_results.append({
            "Model A": names[i],
            "Model B": names[j],
            "Adjusted Rand Index": round(ari, 4)
        })

df_ari = pd.DataFrame(ari_results)
display(df_ari)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ---------- A) Combine correlation and ARI results ----------
summary_df = df_corr.merge(df_ari, on=["Model A", "Model B"], how="outer")

# ---------- B) Display combined results ----------
print("ðŸ“Š Combined Model Comparison Summary:")
display(summary_df)

# ---------- C) Plot correlations ----------
plt.figure(figsize=(6,4))
sns.barplot(x="Model A", y="Spearman Correlation", hue="Model B", data=summary_df)
plt.title("Model Embedding Correlation Comparison")
plt.ylim(0,1)
plt.legend(title="Model B")
plt.show()

# ---------- D) Plot Adjusted Rand Index ----------
plt.figure(figsize=(6,4))
sns.barplot(x="Model A", y="Adjusted Rand Index", hue="Model B", data=summary_df)
plt.title("Clustering Similarity (ARI) Between Models")
plt.ylim(0,1)
plt.legend(title="Model B")
plt.show()

import nbformat

# Path to current notebook
path = '/milestone1_notebook.ipynb'  # change the name if yours is different

# Load notebook
with open(path, 'r', encoding='utf-8') as f:
    nb = nbformat.read(f, as_version=4)

# Remove widget metadata safely
if 'widgets' in nb.get('metadata', {}):
    del nb['metadata']['widgets']

for cell in nb['cells']:
    if 'metadata' in cell and 'widgets' in cell['metadata']:
        del cell['metadata']['widgets']

# Save cleaned copy
clean_path = '/content/milestone1_clean.ipynb'
with open(clean_path, 'w', encoding='utf-8') as f:
    nbformat.write(nb, f)

print("âœ… Clean notebook saved as milestone1_clean.ipynb")

import nbformat

input_path = '/milestone1_notebook.ipynb'   # or your notebook name
output_path = '/content/milestone1_notebook.ipynb'

with open(input_path, 'r', encoding='utf-8') as f:
    nb = nbformat.read(f, as_version=4)

# Loop through every cell and clean widget outputs
for cell in nb['cells']:
    if 'outputs' in cell:
        new_outputs = []
        for output in cell['outputs']:
            if output.get('data') and 'application/vnd.jupyter.widget-view+json' in output['data']:
                continue  # Skip widget outputs
            new_outputs.append(output)
        cell['outputs'] = new_outputs
    # Also clear any widget metadata
    if 'metadata' in cell and 'widgets' in cell['metadata']:
        del cell['metadata']['widgets']

# Remove any global widget metadata
if 'widgets' in nb.get('metadata', {}):
    del nb['metadata']['widgets']

with open(output_path, 'w', encoding='utf-8') as f:
    nbformat.write(nb, f)

print("âœ… Cleaned notebook saved as:", output_path)